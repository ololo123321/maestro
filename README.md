Обучение языковой модели на midi-файлах записей классических произведений:
https://magenta.tensorflow.org/

### Структура проекта:

```
maestro
  maestro-v2.0.0  // распакованный архив с midi-файлами  
  model_data  
    v1  // без токенов педали  
      train_tokens.json  // List[List[str]] - токены событий   
      test_tokens.json  
      eval_tokens.json  
      vocab.json  // List[str] - словарь токенов  
    v2  // с токенами педали; структура как в v1    
  TF1  // реализация проекта на TF 1.14    
    midi  // сгенерированные файлы    
      dpa_4l_8h_64hd_512dff  // названия соответствуют названиям в models    
        0_init.mid  // {индекс}_{init-начальная последовательность, sample-продолжение модели}.mid   
        0_sample.mid   
        ...  
      ra_4l_8h_64hd_512dff  
      lstm_3l_256h  
    models  
      dpa_4l_8h_64hd_512dff  // multi-head dot-product attention: [4]  
      ra_4l_8h_64hd_512dff  // relative attention: [3]  
      lstm_3l_256h  // stacked lstm  
    dot_product_attention.ipynb  
    recurrent.ipynb  
    relative_attention.ipynb  
    generation.ipynb  // код для генерации  
    maestro.py  // основной код  
  TF2  // реализация проекта на TF 2.0.0; структура как в TF1, только переделаны наиболее удачные модели  
  preprocessing.ipynb  // maestro-v2.0.0 -> model_data  
  utils.py  // обработка данных   
  README  
```

### Краткое описание подхода и результатов:  
1. **Препроцессинг**  
Последовательность нот в MIDI-файлах преобразовывалась в последовательность событий, как было предложено в [1] (NOTE_ON_{0:128} (высота), NOTE_OFF_{0:128}, SET_VELOCITY_{0:32} (бинаризованная громкость), TIME_SHIFT_{10,20,...,1000} (в мс)). В первоначальном варианте (model_data/v1) были игнорированы ControlChange's, позволяющие получить информацию о нажатии педалей. Затем было обнаружно, что модель не достаточно хорошо предсказывает события NOTE_OFF_*, что влекло к потере нот при декодировании (нота не записывалась в MIDI-файл, если для неё не получалось вывести момент окончания). Введение событий PEDAL_ON, PEDAL_OFF сильно помогло в устранении этой проблемы, о чём опять же говорится в [1] (model_data/v2). Логика следующая: во время нажатой педали событие NOTE_OFF_i откладывается до наступления самого раннего из следующих событий: PEDAL_OFF, NOTE_ON_i. При переводе токенов в ноты токены PEDAL_{ON, OFF} игнорировались.
2. **Модели**  
    Было опробировано три типа моделей:  
    * stacked lstm ([1])
    * transformer with dot-droduct attention ([2])
    * transformer with relative attention ([2], реализация relative positional representations (RPR) как в [3])  

    Все модели обучались как языковые модели: x - последовательность токенов, y - x, сдвинутый на один токен влево.
    Последовательности для обучения сэмплировались случайным образом фиксированной длины 512 из исходных последовательностей.
    Выбранные для обучения подпоследовательности предобрабатывались: а) удалялись токены NOTE_OFF_*, PEDAL_OFF, для которых нет соответствующих токенов начала; б) производилась аугментация по предложенной в [1] схеме (преобразование высот и временных интервалов).
    При декодировании лучшие результаты удалось поулчить путём сэмплирования токена на каждом шаге из распределения, предсказанного моделью.
    Генерация производилась как с целью продолжить какой-то кусок какой-то композиций, не участвующей в обучении, так и построить мелодию от одной фиксированной ноты или трезвучия.
3. **Результат**  
* lstm_3l_256h: ~2.55*
* ra_4l_8h_64hd_512dff: ~2.2
* dpa_4l_8h_64hd_512dff: ~2.2
* dpa_6l_8h_64hd_1024dff: ~2.05  
Метрика - log-loss.  
Лучшая модель (dot-product attention, 6 слоёв декодера, 8 голов, размерность голов 64, размерность feed-forward слоя в каждом слое декодера 1024) показала log-loss 2.05 на отложенной выборке.
Первые 3 модели реализованы на TF1, последние 2 - на TF2 (из тех соображений, что первая сходится хуже, а вторая весит на порядок больше из-за RPR)

### Reference
[1] https://arxiv.org/abs/1808.03715  
[2] https://arxiv.org/abs/1809.04281  
[3] https://arxiv.org/abs/1803.02155  
[4] https://arxiv.org/abs/1706.03762  